require! {
# -------------------------------------------------------------------------- 3rd
	_: lodash
}



rangeToLoc = -> {start: it.0, end: it.1}

/**
 * Returns a list of ranges associated to a style.
 */
highlight = (ast) ->
	{tokens ? [], comments ? []} = ast

	tokens = [{style: token.type} <<< rangeToLoc token.range for token in tokens]
	comments = [{style: 'Comment'} <<< rangeToLoc comment.range for comment in comments]

	{highlights: _.sortBy (tokens ++ comments), (.start)}


/**
 * Tokenizes a source code.
 *
 * When the whole source is given, indexes of tokens are good, but only those starting at index and for the given length are necessary.
 *
 * When not the whole source is given, for now it's broken. In theory, only the required tokens are computed. But anayway, the starting positions have to be fixed regarding the given index.
 *
 * @param[in] wholeSource Whether the given source represents the whole source of the program or not.
 */
tokenize = (ast, options) ->
		# Pre-processing -------------------------------------------------------

		{tokens, comments} = ast

		tokens ?= []
		comments ?= []

		{source, offset, end, wholeSource} = options

		offset ?= 0
		offset = offset >? 0
		end ?= source.length
		end = end <? source.length
		wholeSource ?= yes

		# Creates our custom token format from what we got ---------------------

		tokens = [{token.type} <<< rangeToLoc token.range for token in tokens]
		comments = [{type: 'Comment'} <<< rangeToLoc comment.range for comment in comments]

		tokens ++= comments
		tokens = _.sortBy tokens, (.start)

		# Adds missing whitespaces ---------------------------------------------

		whitespaces = []
		lastToken = {end: 0}
		for token in tokens
			if token.start > lastToken.end => whitespaces.push {
				type: 'ws'
				start: lastToken.end
				end: token.start
			}
			lastToken = token

		lastToken = tokens[*-1]
		if lastToken.end < source.length => whitespaces.push {
			type: 'ws'
			start: lastToken.end
			end: source.length
		}

		tokens ++= whitespaces
		tokens = _.sortBy tokens, (.start)

		# Applies offset or extract tokens -------------------------------------

		if not wholeSource
			for token in tokens
				token.start += offset
				token.end += offset
		else
			start = offset

			# first = 0
			first = tokens.length - 1
			for token, index in tokens
				if token.start is start
					first = index
					break
				if token.start > start
					first = index - 1
					break

			last = tokens.length - 1
			for token, index in tokens[first to]
				if token.end >= end
					last = index + first
					break

			tokens = tokens[first to last]

			# Cut these extreme tokens
			tokens.0.start = offset
			tokens[*-1].end = end

		# Return
		{tokens}


exports <<< {
	highlight
	tokenize
}
